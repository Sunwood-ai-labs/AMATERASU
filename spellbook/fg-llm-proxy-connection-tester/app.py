import streamlit as st
import openai
import json
import os
import socket
import dns.resolver
import requests
import os

def get_ip_info():
    # ãƒ‘ãƒ–ãƒªãƒƒã‚¯IPã®å–å¾—
    try:
        public_ip = requests.get('https://api.ipify.org').text
    except:
        public_ip = "å–å¾—å¤±æ•—"
    
    # ãƒ­ãƒ¼ã‚«ãƒ«IPã®å–å¾—
    try:
        hostname = socket.gethostname()
        local_ip = socket.gethostbyname(hostname)
    except:
        local_ip = "å–å¾—å¤±æ•—"
        
    return {
        "ãƒ‘ãƒ–ãƒªãƒƒã‚¯IP": public_ip,
        "ãƒ­ãƒ¼ã‚«ãƒ«IP": local_ip,
        "ãƒ›ã‚¹ãƒˆå": hostname
    }

def get_global_accelerator_ip():
    dns_name = os.environ.get('GLOBAL_ACCELERATOR_DNS_NAME')
    if dns_name:
        resolver = dns.resolver.Resolver()
        try:
            ip_addresses = [str(ip.address) for ip in resolver.resolve(dns_name, 'A')]
            ip_address_str = ", ".join(ip_addresses)
            return f"DNS: {dns_name}\nIPã‚¢ãƒ‰ãƒ¬ã‚¹: {ip_address_str}"
        except Exception as e:
            print(f"DNS resolution failed: {e}")
            return f"å–å¾—å¤±æ•—: {str(e)}"
    return "Global Accelerator DNSåãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
    
def main():
    st.set_page_config(page_title="llm-tester", layout="wide")
    st.title("ğŸš€ llm-tester v0.1")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®šé …ç›®ã‚’é…ç½®
    with st.sidebar:
        st.header("ğŸ› ï¸ è¨­å®š")
        base_url = st.text_input("LiteLLM Proxy URL", "http://0.0.0.0:4000")
        api_key = st.text_input("API Key", "your_api_key", type="password")
        model = st.text_input("ãƒ¢ãƒ‡ãƒ«å", "gpt-4o-mini")
        max_tokens = st.number_input("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°", min_value=1, value=1000)
        temperature = st.slider("Temperature", min_value=0.0, max_value=2.0, value=1.0, step=0.1)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
        st.header("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
        ip_info = get_ip_info()
        for key, value in ip_info.items():
            st.text(f"{key}: {value}")

        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¡¨ç¤º
        st.header("ğŸŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿æƒ…å ±")
        global_accelerator_ip = get_global_accelerator_ip()
        st.text(f"IPã‚¢ãƒ‰ãƒ¬ã‚¹: {global_accelerator_ip}")

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã¨çµæœè¡¨ç¤º
    prompt = st.text_area("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=200)
    
    if st.button("é€ä¿¡"):
        if not prompt:
            st.warning("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
            
        try:
            with st.spinner("å‡¦ç†ä¸­..."):
                # OpenAI clientã®è¨­å®š
                client = openai.OpenAI(
                    api_key=api_key,
                    base_url=base_url
                )
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ
                response = client.chat.completions.create(
                    model=model,
                    messages=[{
                        "role": "user",
                        "content": prompt
                    }],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                # çµæœã®è¡¨ç¤º
                st.subheader("ğŸ¤– å¿œç­”")
                st.markdown(response.choices[0].message.content)
                
                # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ã‚’è¡¨ç¤º
                with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“"):
                    st.code(json.dumps(response.model_dump(), indent=2, ensure_ascii=False), language="json")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

if __name__ == "__main__":
    main()
