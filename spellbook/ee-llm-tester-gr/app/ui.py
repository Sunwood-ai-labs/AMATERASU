"""UIé–¢é€£ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ"""

import gradio as gr
from typing import Tuple
from app.models import MODEL_PRESETS, load_preset
from app.utils import get_ip_info

def create_ui(process_prompt_fn) -> gr.Blocks:
    """Gradio UIã®ä½œæˆ"""
    with gr.Blocks(title="LLM Tester", theme=gr.themes.Ocean()) as interface:
        gr.Markdown("# ğŸš€ LLM Tester v0.2")
        
        with gr.Row():
            with gr.Column(scale=2):
                # ãƒ¡ã‚¤ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢
                prompt_input = gr.TextArea(
                    label="ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›",
                    placeholder="ãƒ†ã‚¹ãƒˆã—ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„...",
                    lines=10
                )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸš€ é€ä¿¡", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢", variant="secondary")
                
                response_output = gr.Markdown(label="å¿œç­”")
                debug_output = gr.Markdown(label="ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
                
            with gr.Column(scale=1):
                with gr.Tab("ãƒ¢ãƒ‡ãƒ«è¨­å®š"):
                    preset_dropdown = gr.Dropdown(
                        choices=list(MODEL_PRESETS.keys()),
                        value="GPT-3.5 Turbo",
                        label="ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆ"
                    )
                    model = gr.Textbox(
                        label="ãƒ¢ãƒ‡ãƒ«å",
                        value="gpt-3.5-turbo",
                        placeholder="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å"
                    )
                    max_tokens = gr.Number(
                        label="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
                        value=1000,
                        minimum=1,
                        maximum=4000
                    )
                    temperature = gr.Slider(
                        label="Temperature",
                        minimum=0.0,
                        maximum=2.0,
                        value=1.0,
                        step=0.1
                    )

                with gr.Tab("æ¥ç¶šè¨­å®š"):
                    base_url = gr.Textbox(
                        label="LiteLLM Proxy URL",
                        value="http://0.0.0.0:4000",
                        placeholder="ä¾‹: http://0.0.0.0:4000"
                    )
                    api_key = gr.Textbox(
                        label="API Key",
                        value="your_api_key",
                        type="password",
                        placeholder="OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›"
                    )

                with gr.Tab("ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"):
                    ip_info = get_ip_info()
                    gr.Markdown("\n".join([
                        f"**{k}**: {v}" for k, v in ip_info.items()
                    ]))
                
                with gr.Tab("ãƒ˜ãƒ«ãƒ—"):
                    gr.Markdown("""
                        ### ä½¿ã„æ–¹
                        1. ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã‹ã€è©³ç´°è¨­å®šã‚’è¡Œã„ã¾ã™
                        2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¾ã™
                        3. é€ä¿¡ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦çµæœã‚’ç¢ºèªã—ã¾ã™
                        
                        ### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
                        - API ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ API Key ã‚’ç¢ºèªã—ã¦ãã ã•ã„
                        - æ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ Proxy URL ã‚’ç¢ºèªã—ã¦ãã ã•ã„
                        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„å ´åˆã¯ max_tokens ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
                    """)
        
        def clear_outputs() -> Tuple[str, str]:
            return ["", ""]
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®š
        preset_dropdown.change(
            fn=load_preset,
            inputs=[preset_dropdown],
            outputs=[model, max_tokens, temperature]
        )
        
        submit_btn.click(
            fn=process_prompt_fn,
            inputs=[
                prompt_input,
                base_url,
                api_key,
                model,
                max_tokens,
                temperature
            ],
            outputs=[
                response_output,
                debug_output
            ]
        )
        
        clear_btn.click(
            fn=clear_outputs,
            inputs=[],
            outputs=[response_output, debug_output]
        )
    
    return interface
